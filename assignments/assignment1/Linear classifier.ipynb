{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.57611688 -0.78805844  0.21194156]\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "print(grad)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 731.590077\n",
      "Epoch 1, loss: 819.230071\n",
      "Epoch 2, loss: 768.923102\n",
      "Epoch 3, loss: 886.002130\n",
      "Epoch 4, loss: 1099.364773\n",
      "Epoch 5, loss: 707.079689\n",
      "Epoch 6, loss: 1113.397138\n",
      "Epoch 7, loss: 768.140792\n",
      "Epoch 8, loss: 743.859065\n",
      "Epoch 9, loss: 1025.141056\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2d6550df978>]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.164\n",
      "Epoch 0, loss: 871.043180\n",
      "Epoch 1, loss: 909.991607\n",
      "Epoch 2, loss: 865.264042\n",
      "Epoch 3, loss: 873.989037\n",
      "Epoch 4, loss: 844.775225\n",
      "Epoch 5, loss: 821.013493\n",
      "Epoch 6, loss: 994.170102\n",
      "Epoch 7, loss: 966.520476\n",
      "Epoch 8, loss: 746.017024\n",
      "Epoch 9, loss: 830.072871\n",
      "Epoch 10, loss: 854.577119\n",
      "Epoch 11, loss: 996.321601\n",
      "Epoch 12, loss: 877.843666\n",
      "Epoch 13, loss: 861.092168\n",
      "Epoch 14, loss: 934.689301\n",
      "Epoch 15, loss: 794.363364\n",
      "Epoch 16, loss: 847.866134\n",
      "Epoch 17, loss: 915.258395\n",
      "Epoch 18, loss: 770.360038\n",
      "Epoch 19, loss: 965.863001\n",
      "Epoch 20, loss: 792.337462\n",
      "Epoch 21, loss: 825.598260\n",
      "Epoch 22, loss: 946.266407\n",
      "Epoch 23, loss: 875.737795\n",
      "Epoch 24, loss: 920.407820\n",
      "Epoch 25, loss: 799.135276\n",
      "Epoch 26, loss: 938.194220\n",
      "Epoch 27, loss: 830.119356\n",
      "Epoch 28, loss: 840.481324\n",
      "Epoch 29, loss: 846.650633\n",
      "Epoch 30, loss: 899.306642\n",
      "Epoch 31, loss: 945.983509\n",
      "Epoch 32, loss: 723.123753\n",
      "Epoch 33, loss: 826.210368\n",
      "Epoch 34, loss: 796.069204\n",
      "Epoch 35, loss: 1031.466181\n",
      "Epoch 36, loss: 752.464145\n",
      "Epoch 37, loss: 929.717093\n",
      "Epoch 38, loss: 1106.442355\n",
      "Epoch 39, loss: 826.759886\n",
      "Epoch 40, loss: 820.727967\n",
      "Epoch 41, loss: 917.881605\n",
      "Epoch 42, loss: 861.158553\n",
      "Epoch 43, loss: 939.265513\n",
      "Epoch 44, loss: 958.850390\n",
      "Epoch 45, loss: 922.952731\n",
      "Epoch 46, loss: 844.728637\n",
      "Epoch 47, loss: 1049.059584\n",
      "Epoch 48, loss: 775.116079\n",
      "Epoch 49, loss: 847.489860\n",
      "Epoch 50, loss: 927.805379\n",
      "Epoch 51, loss: 863.550117\n",
      "Epoch 52, loss: 856.345691\n",
      "Epoch 53, loss: 949.533923\n",
      "Epoch 54, loss: 1053.861456\n",
      "Epoch 55, loss: 819.809911\n",
      "Epoch 56, loss: 883.670747\n",
      "Epoch 57, loss: 876.485566\n",
      "Epoch 58, loss: 885.505908\n",
      "Epoch 59, loss: 892.590670\n",
      "Epoch 60, loss: 870.539471\n",
      "Epoch 61, loss: 920.348634\n",
      "Epoch 62, loss: 793.559814\n",
      "Epoch 63, loss: 754.496975\n",
      "Epoch 64, loss: 932.839007\n",
      "Epoch 65, loss: 753.704306\n",
      "Epoch 66, loss: 881.797890\n",
      "Epoch 67, loss: 946.332897\n",
      "Epoch 68, loss: 1006.319907\n",
      "Epoch 69, loss: 823.057146\n",
      "Epoch 70, loss: 915.783296\n",
      "Epoch 71, loss: 715.711047\n",
      "Epoch 72, loss: 1005.394528\n",
      "Epoch 73, loss: 958.945068\n",
      "Epoch 74, loss: 751.943064\n",
      "Epoch 75, loss: 1082.872379\n",
      "Epoch 76, loss: 751.863251\n",
      "Epoch 77, loss: 993.885683\n",
      "Epoch 78, loss: 1134.371915\n",
      "Epoch 79, loss: 1028.965799\n",
      "Epoch 80, loss: 841.351830\n",
      "Epoch 81, loss: 1019.392796\n",
      "Epoch 82, loss: 794.162937\n",
      "Epoch 83, loss: 760.512278\n",
      "Epoch 84, loss: 836.575233\n",
      "Epoch 85, loss: 935.523835\n",
      "Epoch 86, loss: 757.458207\n",
      "Epoch 87, loss: 826.285985\n",
      "Epoch 88, loss: 960.361543\n",
      "Epoch 89, loss: 756.566353\n",
      "Epoch 90, loss: 1025.879363\n",
      "Epoch 91, loss: 1052.820593\n",
      "Epoch 92, loss: 773.173549\n",
      "Epoch 93, loss: 827.879403\n",
      "Epoch 94, loss: 801.872556\n",
      "Epoch 95, loss: 879.418293\n",
      "Epoch 96, loss: 771.672972\n",
      "Epoch 97, loss: 814.038122\n",
      "Epoch 98, loss: 857.385682\n",
      "Epoch 99, loss: 732.892116\n",
      "Accuracy after training for 100 epochs:  0.173\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 1180.837983\n",
      "Epoch 1, loss: 857.413464\n",
      "Epoch 2, loss: 707.729975\n",
      "Epoch 3, loss: 906.891009\n",
      "Epoch 4, loss: 902.788850\n",
      "Epoch 5, loss: 835.835302\n",
      "Epoch 6, loss: 786.280979\n",
      "Epoch 7, loss: 829.595247\n",
      "Epoch 8, loss: 845.971782\n",
      "Epoch 9, loss: 774.206048\n",
      "Epoch 10, loss: 783.082337\n",
      "Epoch 11, loss: 827.550860\n",
      "Epoch 12, loss: 890.909178\n",
      "Epoch 13, loss: 966.551548\n",
      "Epoch 14, loss: 769.994991\n",
      "Epoch 15, loss: 796.295096\n",
      "Epoch 16, loss: 908.348265\n",
      "Epoch 17, loss: 726.810788\n",
      "Epoch 18, loss: 788.305552\n",
      "Epoch 19, loss: 911.919415\n",
      "Epoch 20, loss: 841.058069\n",
      "Epoch 21, loss: 807.193419\n",
      "Epoch 22, loss: 884.998934\n",
      "Epoch 23, loss: 978.232592\n",
      "Epoch 24, loss: 1103.514483\n",
      "Epoch 25, loss: 789.665892\n",
      "Epoch 26, loss: 799.446644\n",
      "Epoch 27, loss: 716.218911\n",
      "Epoch 28, loss: 957.207435\n",
      "Epoch 29, loss: 717.671370\n",
      "Epoch 30, loss: 741.105182\n",
      "Epoch 31, loss: 864.581912\n",
      "Epoch 32, loss: 658.371586\n",
      "Epoch 33, loss: 704.563331\n",
      "Epoch 34, loss: 690.439707\n",
      "Epoch 35, loss: 878.898670\n",
      "Epoch 36, loss: 958.228581\n",
      "Epoch 37, loss: 835.662740\n",
      "Epoch 38, loss: 711.498320\n",
      "Epoch 39, loss: 652.036637\n",
      "Epoch 40, loss: 825.702005\n",
      "Epoch 41, loss: 850.981596\n",
      "Epoch 42, loss: 836.935874\n",
      "Epoch 43, loss: 688.412848\n",
      "Epoch 44, loss: 673.207556\n",
      "Epoch 45, loss: 732.988110\n",
      "Epoch 46, loss: 755.855708\n",
      "Epoch 47, loss: 812.437279\n",
      "Epoch 48, loss: 842.678798\n",
      "Epoch 49, loss: 770.287634\n",
      "Epoch 50, loss: 747.413188\n",
      "Epoch 51, loss: 808.685762\n",
      "Epoch 52, loss: 774.053218\n",
      "Epoch 53, loss: 932.844106\n",
      "Epoch 54, loss: 767.223958\n",
      "Epoch 55, loss: 682.665230\n",
      "Epoch 56, loss: 941.659157\n",
      "Epoch 57, loss: 752.111857\n",
      "Epoch 58, loss: 973.517970\n",
      "Epoch 59, loss: 760.170442\n",
      "Epoch 60, loss: 682.399848\n",
      "Epoch 61, loss: 727.801173\n",
      "Epoch 62, loss: 787.543753\n",
      "Epoch 63, loss: 703.030619\n",
      "Epoch 64, loss: 719.997930\n",
      "Epoch 65, loss: 786.536290\n",
      "Epoch 66, loss: 655.616749\n",
      "Epoch 67, loss: 713.816264\n",
      "Epoch 68, loss: 868.587258\n",
      "Epoch 69, loss: 708.318352\n",
      "Epoch 70, loss: 717.660535\n",
      "Epoch 71, loss: 779.495095\n",
      "Epoch 72, loss: 702.250114\n",
      "Epoch 73, loss: 819.523104\n",
      "Epoch 74, loss: 897.520494\n",
      "Epoch 75, loss: 862.211987\n",
      "Epoch 76, loss: 768.325729\n",
      "Epoch 77, loss: 776.892040\n",
      "Epoch 78, loss: 882.728187\n",
      "Epoch 79, loss: 807.852843\n",
      "Epoch 80, loss: 836.278647\n",
      "Epoch 81, loss: 755.385264\n",
      "Epoch 82, loss: 714.302623\n",
      "Epoch 83, loss: 717.912811\n",
      "Epoch 84, loss: 675.505544\n",
      "Epoch 85, loss: 991.663582\n",
      "Epoch 86, loss: 867.934443\n",
      "Epoch 87, loss: 871.695648\n",
      "Epoch 88, loss: 679.517237\n",
      "Epoch 89, loss: 704.255124\n",
      "Epoch 90, loss: 856.764498\n",
      "Epoch 91, loss: 960.026891\n",
      "Epoch 92, loss: 616.659805\n",
      "Epoch 93, loss: 908.010134\n",
      "Epoch 94, loss: 760.864613\n",
      "Epoch 95, loss: 784.968481\n",
      "Epoch 96, loss: 662.466507\n",
      "Epoch 97, loss: 786.725263\n",
      "Epoch 98, loss: 707.392818\n",
      "Epoch 99, loss: 784.793830\n",
      "Epoch 100, loss: 773.293974\n",
      "Epoch 101, loss: 762.739661\n",
      "Epoch 102, loss: 666.726272\n",
      "Epoch 103, loss: 697.600536\n",
      "Epoch 104, loss: 819.653053\n",
      "Epoch 105, loss: 681.597513\n",
      "Epoch 106, loss: 789.881058\n",
      "Epoch 107, loss: 893.205951\n",
      "Epoch 108, loss: 687.035533\n",
      "Epoch 109, loss: 807.421182\n",
      "Epoch 110, loss: 668.585785\n",
      "Epoch 111, loss: 701.174471\n",
      "Epoch 112, loss: 901.921911\n",
      "Epoch 113, loss: 679.580022\n",
      "Epoch 114, loss: 664.125366\n",
      "Epoch 115, loss: 814.048379\n",
      "Epoch 116, loss: 947.543636\n",
      "Epoch 117, loss: 716.702471\n",
      "Epoch 118, loss: 714.494230\n",
      "Epoch 119, loss: 828.577759\n",
      "Epoch 120, loss: 712.710276\n",
      "Epoch 121, loss: 715.842159\n",
      "Epoch 122, loss: 901.151189\n",
      "Epoch 123, loss: 623.846959\n",
      "Epoch 124, loss: 754.476165\n",
      "Epoch 125, loss: 725.989165\n",
      "Epoch 126, loss: 695.818438\n",
      "Epoch 127, loss: 751.787652\n",
      "Epoch 128, loss: 716.671546\n",
      "Epoch 129, loss: 657.699454\n",
      "Epoch 130, loss: 715.483640\n",
      "Epoch 131, loss: 686.992124\n",
      "Epoch 132, loss: 751.528245\n",
      "Epoch 133, loss: 934.651871\n",
      "Epoch 134, loss: 763.084557\n",
      "Epoch 135, loss: 756.321793\n",
      "Epoch 136, loss: 659.435633\n",
      "Epoch 137, loss: 698.190295\n",
      "Epoch 138, loss: 714.140503\n",
      "Epoch 139, loss: 669.775817\n",
      "Epoch 140, loss: 711.909100\n",
      "Epoch 141, loss: 776.493773\n",
      "Epoch 142, loss: 761.138826\n",
      "Epoch 143, loss: 766.739125\n",
      "Epoch 144, loss: 748.279221\n",
      "Epoch 145, loss: 640.726911\n",
      "Epoch 146, loss: 695.322167\n",
      "Epoch 147, loss: 662.198155\n",
      "Epoch 148, loss: 916.879283\n",
      "Epoch 149, loss: 879.235362\n",
      "Epoch 150, loss: 823.701861\n",
      "Epoch 151, loss: 765.820879\n",
      "Epoch 152, loss: 795.490647\n",
      "Epoch 153, loss: 771.608972\n",
      "Epoch 154, loss: 859.544295\n",
      "Epoch 155, loss: 838.788133\n",
      "Epoch 156, loss: 690.713395\n",
      "Epoch 157, loss: 804.992644\n",
      "Epoch 158, loss: 812.378348\n",
      "Epoch 159, loss: 670.958395\n",
      "Epoch 160, loss: 801.225342\n",
      "Epoch 161, loss: 705.295360\n",
      "Epoch 162, loss: 718.722133\n",
      "Epoch 163, loss: 790.419118\n",
      "Epoch 164, loss: 777.269580\n",
      "Epoch 165, loss: 689.323017\n",
      "Epoch 166, loss: 738.575313\n",
      "Epoch 167, loss: 906.967837\n",
      "Epoch 168, loss: 781.364464\n",
      "Epoch 169, loss: 677.349986\n",
      "Epoch 170, loss: 687.092657\n",
      "Epoch 171, loss: 655.935161\n",
      "Epoch 172, loss: 772.174630\n",
      "Epoch 173, loss: 875.926239\n",
      "Epoch 174, loss: 690.119177\n",
      "Epoch 175, loss: 649.623970\n",
      "Epoch 176, loss: 715.046631\n",
      "Epoch 177, loss: 747.659601\n",
      "Epoch 178, loss: 813.910446\n",
      "Epoch 179, loss: 766.144450\n",
      "Epoch 180, loss: 893.326973\n",
      "Epoch 181, loss: 713.242231\n",
      "Epoch 182, loss: 688.443176\n",
      "Epoch 183, loss: 753.883260\n",
      "Epoch 184, loss: 691.962756\n",
      "Epoch 185, loss: 748.559701\n",
      "Epoch 186, loss: 810.043105\n",
      "Epoch 187, loss: 717.333283\n",
      "Epoch 188, loss: 650.418801\n",
      "Epoch 189, loss: 757.718759\n",
      "Epoch 190, loss: 739.446815\n",
      "Epoch 191, loss: 986.790036\n",
      "Epoch 192, loss: 754.169880\n",
      "Epoch 193, loss: 771.216630\n",
      "Epoch 194, loss: 729.739022\n",
      "Epoch 195, loss: 853.618052\n",
      "Epoch 196, loss: 706.921835\n",
      "Epoch 197, loss: 864.145867\n",
      "Epoch 198, loss: 740.624164\n",
      "Epoch 199, loss: 704.483280\n",
      "Epoch 0, loss: 740.767597\n",
      "Epoch 1, loss: 758.881644\n",
      "Epoch 2, loss: 768.054150\n",
      "Epoch 3, loss: 777.219885\n",
      "Epoch 4, loss: 778.798595\n",
      "Epoch 5, loss: 709.349203\n",
      "Epoch 6, loss: 838.679328\n",
      "Epoch 7, loss: 723.358316\n",
      "Epoch 8, loss: 724.276070\n",
      "Epoch 9, loss: 685.456964\n",
      "Epoch 10, loss: 713.107554\n",
      "Epoch 11, loss: 614.656039\n",
      "Epoch 12, loss: 686.906246\n",
      "Epoch 13, loss: 603.242816\n",
      "Epoch 14, loss: 779.998850\n",
      "Epoch 15, loss: 652.287304\n",
      "Epoch 16, loss: 754.466788\n",
      "Epoch 17, loss: 853.014730\n",
      "Epoch 18, loss: 636.364592\n",
      "Epoch 19, loss: 800.695748\n",
      "Epoch 20, loss: 816.653931\n",
      "Epoch 21, loss: 696.085500\n",
      "Epoch 22, loss: 661.003019\n",
      "Epoch 23, loss: 775.628631\n",
      "Epoch 24, loss: 719.288804\n",
      "Epoch 25, loss: 644.533456\n",
      "Epoch 26, loss: 618.350923\n",
      "Epoch 27, loss: 690.165559\n",
      "Epoch 28, loss: 719.643132\n",
      "Epoch 29, loss: 713.563527\n",
      "Epoch 30, loss: 635.852470\n",
      "Epoch 31, loss: 822.294595\n",
      "Epoch 32, loss: 750.765357\n",
      "Epoch 33, loss: 866.407714\n",
      "Epoch 34, loss: 756.335156\n",
      "Epoch 35, loss: 643.232985\n",
      "Epoch 36, loss: 799.435498\n",
      "Epoch 37, loss: 809.736546\n",
      "Epoch 38, loss: 739.444646\n",
      "Epoch 39, loss: 674.163517\n",
      "Epoch 40, loss: 678.583154\n",
      "Epoch 41, loss: 669.999548\n",
      "Epoch 42, loss: 709.911179\n",
      "Epoch 43, loss: 615.526049\n",
      "Epoch 44, loss: 698.977868\n",
      "Epoch 45, loss: 608.167649\n",
      "Epoch 46, loss: 750.938297\n",
      "Epoch 47, loss: 711.181209\n",
      "Epoch 48, loss: 686.164637\n",
      "Epoch 49, loss: 777.867544\n",
      "Epoch 50, loss: 784.895245\n",
      "Epoch 51, loss: 691.542130\n",
      "Epoch 52, loss: 745.945657\n",
      "Epoch 53, loss: 691.685604\n",
      "Epoch 54, loss: 751.477941\n",
      "Epoch 55, loss: 681.217957\n",
      "Epoch 56, loss: 708.437275\n",
      "Epoch 57, loss: 816.248367\n",
      "Epoch 58, loss: 777.485038\n",
      "Epoch 59, loss: 681.662785\n",
      "Epoch 60, loss: 699.399849\n",
      "Epoch 61, loss: 689.354612\n",
      "Epoch 62, loss: 616.791838\n",
      "Epoch 63, loss: 732.536384\n",
      "Epoch 64, loss: 774.529986\n",
      "Epoch 65, loss: 686.606959\n",
      "Epoch 66, loss: 608.155999\n",
      "Epoch 67, loss: 693.990428\n",
      "Epoch 68, loss: 677.082130\n",
      "Epoch 69, loss: 735.288828\n",
      "Epoch 70, loss: 772.785283\n",
      "Epoch 71, loss: 742.902327\n",
      "Epoch 72, loss: 715.347452\n",
      "Epoch 73, loss: 694.051277\n",
      "Epoch 74, loss: 674.768241\n",
      "Epoch 75, loss: 767.267647\n",
      "Epoch 76, loss: 646.877853\n",
      "Epoch 77, loss: 664.777233\n",
      "Epoch 78, loss: 732.838212\n",
      "Epoch 79, loss: 637.686553\n",
      "Epoch 80, loss: 679.335014\n",
      "Epoch 81, loss: 679.551165\n",
      "Epoch 82, loss: 695.558772\n",
      "Epoch 83, loss: 632.472239\n",
      "Epoch 84, loss: 707.168729\n",
      "Epoch 85, loss: 592.071129\n",
      "Epoch 86, loss: 632.996115\n",
      "Epoch 87, loss: 644.301242\n",
      "Epoch 88, loss: 645.578016\n",
      "Epoch 89, loss: 808.706374\n",
      "Epoch 90, loss: 904.494337\n",
      "Epoch 91, loss: 651.313412\n",
      "Epoch 92, loss: 859.438604\n",
      "Epoch 93, loss: 679.083266\n",
      "Epoch 94, loss: 620.210486\n",
      "Epoch 95, loss: 716.949943\n",
      "Epoch 96, loss: 755.798970\n",
      "Epoch 97, loss: 727.933133\n",
      "Epoch 98, loss: 749.309187\n",
      "Epoch 99, loss: 667.372850\n",
      "Epoch 100, loss: 801.433691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101, loss: 662.244648\n",
      "Epoch 102, loss: 680.683394\n",
      "Epoch 103, loss: 739.028377\n",
      "Epoch 104, loss: 780.452340\n",
      "Epoch 105, loss: 647.584898\n",
      "Epoch 106, loss: 818.271025\n",
      "Epoch 107, loss: 999.619081\n",
      "Epoch 108, loss: 797.427676\n",
      "Epoch 109, loss: 764.014142\n",
      "Epoch 110, loss: 764.022931\n",
      "Epoch 111, loss: 671.037047\n",
      "Epoch 112, loss: 828.487238\n",
      "Epoch 113, loss: 710.222243\n",
      "Epoch 114, loss: 692.266043\n",
      "Epoch 115, loss: 838.365858\n",
      "Epoch 116, loss: 734.509457\n",
      "Epoch 117, loss: 736.650858\n",
      "Epoch 118, loss: 716.903694\n",
      "Epoch 119, loss: 740.780257\n",
      "Epoch 120, loss: 831.586677\n",
      "Epoch 121, loss: 871.235962\n",
      "Epoch 122, loss: 700.832572\n",
      "Epoch 123, loss: 651.129340\n",
      "Epoch 124, loss: 779.547274\n",
      "Epoch 125, loss: 718.182427\n",
      "Epoch 126, loss: 835.124438\n",
      "Epoch 127, loss: 779.449776\n",
      "Epoch 128, loss: 628.995091\n",
      "Epoch 129, loss: 617.986738\n",
      "Epoch 130, loss: 719.762995\n",
      "Epoch 131, loss: 650.061212\n",
      "Epoch 132, loss: 726.868271\n",
      "Epoch 133, loss: 618.604443\n",
      "Epoch 134, loss: 749.563634\n",
      "Epoch 135, loss: 726.537303\n",
      "Epoch 136, loss: 653.859336\n",
      "Epoch 137, loss: 818.682146\n",
      "Epoch 138, loss: 711.523554\n",
      "Epoch 139, loss: 693.427399\n",
      "Epoch 140, loss: 793.173145\n",
      "Epoch 141, loss: 771.047568\n",
      "Epoch 142, loss: 773.913260\n",
      "Epoch 143, loss: 717.551529\n",
      "Epoch 144, loss: 628.884104\n",
      "Epoch 145, loss: 766.073566\n",
      "Epoch 146, loss: 750.286644\n",
      "Epoch 147, loss: 710.274005\n",
      "Epoch 148, loss: 766.459335\n",
      "Epoch 149, loss: 664.477855\n",
      "Epoch 150, loss: 759.347775\n",
      "Epoch 151, loss: 623.795437\n",
      "Epoch 152, loss: 671.587208\n",
      "Epoch 153, loss: 597.408060\n",
      "Epoch 154, loss: 640.450217\n",
      "Epoch 155, loss: 649.350555\n",
      "Epoch 156, loss: 660.394874\n",
      "Epoch 157, loss: 717.487993\n",
      "Epoch 158, loss: 821.731390\n",
      "Epoch 159, loss: 659.613972\n",
      "Epoch 160, loss: 748.607991\n",
      "Epoch 161, loss: 681.684288\n",
      "Epoch 162, loss: 684.412062\n",
      "Epoch 163, loss: 724.683423\n",
      "Epoch 164, loss: 643.223475\n",
      "Epoch 165, loss: 696.347727\n",
      "Epoch 166, loss: 638.035008\n",
      "Epoch 167, loss: 870.693200\n",
      "Epoch 168, loss: 709.186756\n",
      "Epoch 169, loss: 638.717388\n",
      "Epoch 170, loss: 606.861436\n",
      "Epoch 171, loss: 666.590716\n",
      "Epoch 172, loss: 852.050776\n",
      "Epoch 173, loss: 704.456480\n",
      "Epoch 174, loss: 864.802237\n",
      "Epoch 175, loss: 744.045739\n",
      "Epoch 176, loss: 639.672053\n",
      "Epoch 177, loss: 732.328325\n",
      "Epoch 178, loss: 685.372879\n",
      "Epoch 179, loss: 712.349403\n",
      "Epoch 180, loss: 729.243095\n",
      "Epoch 181, loss: 687.510107\n",
      "Epoch 182, loss: 793.133819\n",
      "Epoch 183, loss: 746.643073\n",
      "Epoch 184, loss: 592.618289\n",
      "Epoch 185, loss: 741.857811\n",
      "Epoch 186, loss: 633.582736\n",
      "Epoch 187, loss: 609.605942\n",
      "Epoch 188, loss: 932.664840\n",
      "Epoch 189, loss: 706.022887\n",
      "Epoch 190, loss: 648.827479\n",
      "Epoch 191, loss: 737.569985\n",
      "Epoch 192, loss: 708.830090\n",
      "Epoch 193, loss: 758.642311\n",
      "Epoch 194, loss: 693.239567\n",
      "Epoch 195, loss: 666.492409\n",
      "Epoch 196, loss: 638.939405\n",
      "Epoch 197, loss: 631.838141\n",
      "Epoch 198, loss: 647.591036\n",
      "Epoch 199, loss: 825.859788\n",
      "Epoch 0, loss: 753.072234\n",
      "Epoch 1, loss: 599.316725\n",
      "Epoch 2, loss: 741.873987\n",
      "Epoch 3, loss: 591.817219\n",
      "Epoch 4, loss: 661.518086\n",
      "Epoch 5, loss: 825.033485\n",
      "Epoch 6, loss: 843.724841\n",
      "Epoch 7, loss: 740.712468\n",
      "Epoch 8, loss: 837.495563\n",
      "Epoch 9, loss: 626.895748\n",
      "Epoch 10, loss: 802.032787\n",
      "Epoch 11, loss: 663.012478\n",
      "Epoch 12, loss: 652.369310\n",
      "Epoch 13, loss: 608.569868\n",
      "Epoch 14, loss: 803.547352\n",
      "Epoch 15, loss: 822.401759\n",
      "Epoch 16, loss: 644.579979\n",
      "Epoch 17, loss: 624.114304\n",
      "Epoch 18, loss: 673.225631\n",
      "Epoch 19, loss: 629.559445\n",
      "Epoch 20, loss: 802.431131\n",
      "Epoch 21, loss: 627.518206\n",
      "Epoch 22, loss: 859.356986\n",
      "Epoch 23, loss: 785.232660\n",
      "Epoch 24, loss: 757.303803\n",
      "Epoch 25, loss: 682.958672\n",
      "Epoch 26, loss: 791.172810\n",
      "Epoch 27, loss: 674.668087\n",
      "Epoch 28, loss: 663.149538\n",
      "Epoch 29, loss: 668.506093\n",
      "Epoch 30, loss: 684.781713\n",
      "Epoch 31, loss: 689.348466\n",
      "Epoch 32, loss: 750.870726\n",
      "Epoch 33, loss: 854.685473\n",
      "Epoch 34, loss: 701.465244\n",
      "Epoch 35, loss: 859.492741\n",
      "Epoch 36, loss: 619.964273\n",
      "Epoch 37, loss: 634.768247\n",
      "Epoch 38, loss: 721.594233\n",
      "Epoch 39, loss: 623.292440\n",
      "Epoch 40, loss: 795.229102\n",
      "Epoch 41, loss: 623.302984\n",
      "Epoch 42, loss: 588.722688\n",
      "Epoch 43, loss: 693.559424\n",
      "Epoch 44, loss: 672.182885\n",
      "Epoch 45, loss: 702.829432\n",
      "Epoch 46, loss: 768.667220\n",
      "Epoch 47, loss: 684.480500\n",
      "Epoch 48, loss: 720.575621\n",
      "Epoch 49, loss: 801.818801\n",
      "Epoch 50, loss: 799.425749\n",
      "Epoch 51, loss: 748.573729\n",
      "Epoch 52, loss: 787.900436\n",
      "Epoch 53, loss: 598.929727\n",
      "Epoch 54, loss: 659.721640\n",
      "Epoch 55, loss: 685.433078\n",
      "Epoch 56, loss: 765.500517\n",
      "Epoch 57, loss: 715.777897\n",
      "Epoch 58, loss: 650.933833\n",
      "Epoch 59, loss: 680.240280\n",
      "Epoch 60, loss: 662.219588\n",
      "Epoch 61, loss: 664.390742\n",
      "Epoch 62, loss: 762.454820\n",
      "Epoch 63, loss: 701.930728\n",
      "Epoch 64, loss: 670.054432\n",
      "Epoch 65, loss: 676.299421\n",
      "Epoch 66, loss: 648.213255\n",
      "Epoch 67, loss: 667.411366\n",
      "Epoch 68, loss: 713.072037\n",
      "Epoch 69, loss: 602.487739\n",
      "Epoch 70, loss: 572.560946\n",
      "Epoch 71, loss: 626.422374\n",
      "Epoch 72, loss: 655.678064\n",
      "Epoch 73, loss: 795.558609\n",
      "Epoch 74, loss: 585.813248\n",
      "Epoch 75, loss: 800.884012\n",
      "Epoch 76, loss: 604.929649\n",
      "Epoch 77, loss: 678.564721\n",
      "Epoch 78, loss: 649.355626\n",
      "Epoch 79, loss: 779.265954\n",
      "Epoch 80, loss: 621.683820\n",
      "Epoch 81, loss: 859.446661\n",
      "Epoch 82, loss: 707.387999\n",
      "Epoch 83, loss: 614.390848\n",
      "Epoch 84, loss: 611.818759\n",
      "Epoch 85, loss: 573.822584\n",
      "Epoch 86, loss: 599.996225\n",
      "Epoch 87, loss: 617.875576\n",
      "Epoch 88, loss: 699.782954\n",
      "Epoch 89, loss: 656.107667\n",
      "Epoch 90, loss: 683.632832\n",
      "Epoch 91, loss: 708.432806\n",
      "Epoch 92, loss: 666.797015\n",
      "Epoch 93, loss: 641.099525\n",
      "Epoch 94, loss: 632.888291\n",
      "Epoch 95, loss: 734.545362\n",
      "Epoch 96, loss: 657.882621\n",
      "Epoch 97, loss: 802.220013\n",
      "Epoch 98, loss: 644.888433\n",
      "Epoch 99, loss: 674.329204\n",
      "Epoch 100, loss: 885.229908\n",
      "Epoch 101, loss: 667.844878\n",
      "Epoch 102, loss: 672.421776\n",
      "Epoch 103, loss: 674.956415\n",
      "Epoch 104, loss: 659.434282\n",
      "Epoch 105, loss: 779.137736\n",
      "Epoch 106, loss: 622.682498\n",
      "Epoch 107, loss: 612.258791\n",
      "Epoch 108, loss: 702.327485\n",
      "Epoch 109, loss: 634.944322\n",
      "Epoch 110, loss: 828.199495\n",
      "Epoch 111, loss: 717.297097\n",
      "Epoch 112, loss: 778.411338\n",
      "Epoch 113, loss: 788.587971\n",
      "Epoch 114, loss: 597.149142\n",
      "Epoch 115, loss: 747.645483\n",
      "Epoch 116, loss: 635.186751\n",
      "Epoch 117, loss: 796.361026\n",
      "Epoch 118, loss: 600.768264\n",
      "Epoch 119, loss: 669.344307\n",
      "Epoch 120, loss: 704.848566\n",
      "Epoch 121, loss: 810.643810\n",
      "Epoch 122, loss: 607.403245\n",
      "Epoch 123, loss: 672.466968\n",
      "Epoch 124, loss: 601.906649\n",
      "Epoch 125, loss: 720.839537\n",
      "Epoch 126, loss: 621.397548\n",
      "Epoch 127, loss: 621.813820\n",
      "Epoch 128, loss: 750.898096\n",
      "Epoch 129, loss: 673.358531\n",
      "Epoch 130, loss: 706.951979\n",
      "Epoch 131, loss: 646.622019\n",
      "Epoch 132, loss: 721.784509\n",
      "Epoch 133, loss: 831.384499\n",
      "Epoch 134, loss: 757.379090\n",
      "Epoch 135, loss: 677.947524\n",
      "Epoch 136, loss: 934.162201\n",
      "Epoch 137, loss: 688.510300\n",
      "Epoch 138, loss: 638.103735\n",
      "Epoch 139, loss: 704.769088\n",
      "Epoch 140, loss: 824.427941\n",
      "Epoch 141, loss: 769.243284\n",
      "Epoch 142, loss: 561.097477\n",
      "Epoch 143, loss: 630.512726\n",
      "Epoch 144, loss: 749.753647\n",
      "Epoch 145, loss: 577.638588\n",
      "Epoch 146, loss: 652.612558\n",
      "Epoch 147, loss: 607.596553\n",
      "Epoch 148, loss: 656.383591\n",
      "Epoch 149, loss: 704.987222\n",
      "Epoch 150, loss: 809.010448\n",
      "Epoch 151, loss: 669.942483\n",
      "Epoch 152, loss: 624.419748\n",
      "Epoch 153, loss: 707.750428\n",
      "Epoch 154, loss: 754.849776\n",
      "Epoch 155, loss: 755.776716\n",
      "Epoch 156, loss: 673.932524\n",
      "Epoch 157, loss: 763.904945\n",
      "Epoch 158, loss: 892.613468\n",
      "Epoch 159, loss: 719.125881\n",
      "Epoch 160, loss: 663.512855\n",
      "Epoch 161, loss: 668.142305\n",
      "Epoch 162, loss: 723.210685\n",
      "Epoch 163, loss: 626.904415\n",
      "Epoch 164, loss: 647.914252\n",
      "Epoch 165, loss: 688.107462\n",
      "Epoch 166, loss: 909.350719\n",
      "Epoch 167, loss: 784.576414\n",
      "Epoch 168, loss: 666.136813\n",
      "Epoch 169, loss: 665.563001\n",
      "Epoch 170, loss: 679.209113\n",
      "Epoch 171, loss: 625.207526\n",
      "Epoch 172, loss: 630.906144\n",
      "Epoch 173, loss: 641.788410\n",
      "Epoch 174, loss: 673.457732\n",
      "Epoch 175, loss: 652.247637\n",
      "Epoch 176, loss: 674.012548\n",
      "Epoch 177, loss: 790.203240\n",
      "Epoch 178, loss: 592.621681\n",
      "Epoch 179, loss: 709.153259\n",
      "Epoch 180, loss: 584.804060\n",
      "Epoch 181, loss: 747.445138\n",
      "Epoch 182, loss: 696.484809\n",
      "Epoch 183, loss: 804.131653\n",
      "Epoch 184, loss: 634.554514\n",
      "Epoch 185, loss: 609.395469\n",
      "Epoch 186, loss: 733.853257\n",
      "Epoch 187, loss: 618.268436\n",
      "Epoch 188, loss: 674.917992\n",
      "Epoch 189, loss: 780.932246\n",
      "Epoch 190, loss: 746.036071\n",
      "Epoch 191, loss: 698.806086\n",
      "Epoch 192, loss: 612.225171\n",
      "Epoch 193, loss: 700.975050\n",
      "Epoch 194, loss: 711.722822\n",
      "Epoch 195, loss: 822.212576\n",
      "Epoch 196, loss: 751.294970\n",
      "Epoch 197, loss: 617.746670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198, loss: 637.934698\n",
      "Epoch 199, loss: 906.606968\n",
      "Epoch 0, loss: 547.127619\n",
      "Epoch 1, loss: 573.369932\n",
      "Epoch 2, loss: 558.321580\n",
      "Epoch 3, loss: 535.970027\n",
      "Epoch 4, loss: 553.891021\n",
      "Epoch 5, loss: 548.562095\n",
      "Epoch 6, loss: 549.032112\n",
      "Epoch 7, loss: 553.493588\n",
      "Epoch 8, loss: 572.548366\n",
      "Epoch 9, loss: 535.289454\n",
      "Epoch 10, loss: 545.502320\n",
      "Epoch 11, loss: 573.020347\n",
      "Epoch 12, loss: 540.669715\n",
      "Epoch 13, loss: 566.798910\n",
      "Epoch 14, loss: 527.136493\n",
      "Epoch 15, loss: 534.268488\n",
      "Epoch 16, loss: 554.218723\n",
      "Epoch 17, loss: 547.775238\n",
      "Epoch 18, loss: 531.935185\n",
      "Epoch 19, loss: 533.465753\n",
      "Epoch 20, loss: 545.964434\n",
      "Epoch 21, loss: 543.616387\n",
      "Epoch 22, loss: 556.358558\n",
      "Epoch 23, loss: 548.206573\n",
      "Epoch 24, loss: 537.421485\n",
      "Epoch 25, loss: 556.058258\n",
      "Epoch 26, loss: 539.244030\n",
      "Epoch 27, loss: 589.027398\n",
      "Epoch 28, loss: 585.270983\n",
      "Epoch 29, loss: 561.039489\n",
      "Epoch 30, loss: 574.293042\n",
      "Epoch 31, loss: 547.930893\n",
      "Epoch 32, loss: 537.911064\n",
      "Epoch 33, loss: 563.342899\n",
      "Epoch 34, loss: 579.561038\n",
      "Epoch 35, loss: 544.777643\n",
      "Epoch 36, loss: 575.114927\n",
      "Epoch 37, loss: 573.880937\n",
      "Epoch 38, loss: 554.800386\n",
      "Epoch 39, loss: 597.318370\n",
      "Epoch 40, loss: 543.113440\n",
      "Epoch 41, loss: 531.508072\n",
      "Epoch 42, loss: 557.815797\n",
      "Epoch 43, loss: 539.860841\n",
      "Epoch 44, loss: 550.406569\n",
      "Epoch 45, loss: 543.849456\n",
      "Epoch 46, loss: 570.281605\n",
      "Epoch 47, loss: 552.267839\n",
      "Epoch 48, loss: 541.999195\n",
      "Epoch 49, loss: 551.074341\n",
      "Epoch 50, loss: 547.370229\n",
      "Epoch 51, loss: 548.353171\n",
      "Epoch 52, loss: 542.527642\n",
      "Epoch 53, loss: 533.523163\n",
      "Epoch 54, loss: 564.251697\n",
      "Epoch 55, loss: 550.404321\n",
      "Epoch 56, loss: 555.202182\n",
      "Epoch 57, loss: 567.710665\n",
      "Epoch 58, loss: 567.709540\n",
      "Epoch 59, loss: 553.813978\n",
      "Epoch 60, loss: 517.709500\n",
      "Epoch 61, loss: 559.711512\n",
      "Epoch 62, loss: 575.923575\n",
      "Epoch 63, loss: 541.979847\n",
      "Epoch 64, loss: 551.148061\n",
      "Epoch 65, loss: 537.397363\n",
      "Epoch 66, loss: 551.586677\n",
      "Epoch 67, loss: 543.735869\n",
      "Epoch 68, loss: 550.691481\n",
      "Epoch 69, loss: 577.480655\n",
      "Epoch 70, loss: 581.841939\n",
      "Epoch 71, loss: 542.731047\n",
      "Epoch 72, loss: 547.887404\n",
      "Epoch 73, loss: 534.587197\n",
      "Epoch 74, loss: 543.602227\n",
      "Epoch 75, loss: 549.003079\n",
      "Epoch 76, loss: 543.423190\n",
      "Epoch 77, loss: 571.302610\n",
      "Epoch 78, loss: 542.836008\n",
      "Epoch 79, loss: 533.947425\n",
      "Epoch 80, loss: 558.025014\n",
      "Epoch 81, loss: 527.109363\n",
      "Epoch 82, loss: 531.390694\n",
      "Epoch 83, loss: 541.675332\n",
      "Epoch 84, loss: 556.454653\n",
      "Epoch 85, loss: 550.607098\n",
      "Epoch 86, loss: 549.918807\n",
      "Epoch 87, loss: 586.434040\n",
      "Epoch 88, loss: 539.002696\n",
      "Epoch 89, loss: 548.507803\n",
      "Epoch 90, loss: 548.474612\n",
      "Epoch 91, loss: 517.941523\n",
      "Epoch 92, loss: 570.403581\n",
      "Epoch 93, loss: 581.236941\n",
      "Epoch 94, loss: 536.579786\n",
      "Epoch 95, loss: 550.045713\n",
      "Epoch 96, loss: 532.795908\n",
      "Epoch 97, loss: 571.144971\n",
      "Epoch 98, loss: 566.168873\n",
      "Epoch 99, loss: 509.868988\n",
      "Epoch 100, loss: 540.568916\n",
      "Epoch 101, loss: 587.521947\n",
      "Epoch 102, loss: 542.149124\n",
      "Epoch 103, loss: 551.597726\n",
      "Epoch 104, loss: 536.791983\n",
      "Epoch 105, loss: 585.743320\n",
      "Epoch 106, loss: 558.843280\n",
      "Epoch 107, loss: 542.477312\n",
      "Epoch 108, loss: 550.323752\n",
      "Epoch 109, loss: 545.215121\n",
      "Epoch 110, loss: 538.537745\n",
      "Epoch 111, loss: 531.995547\n",
      "Epoch 112, loss: 537.222953\n",
      "Epoch 113, loss: 547.422594\n",
      "Epoch 114, loss: 558.033112\n",
      "Epoch 115, loss: 555.544041\n",
      "Epoch 116, loss: 550.532667\n",
      "Epoch 117, loss: 553.707635\n",
      "Epoch 118, loss: 553.517397\n",
      "Epoch 119, loss: 526.425261\n",
      "Epoch 120, loss: 561.749201\n",
      "Epoch 121, loss: 541.612961\n",
      "Epoch 122, loss: 555.527523\n",
      "Epoch 123, loss: 557.327528\n",
      "Epoch 124, loss: 549.304241\n",
      "Epoch 125, loss: 542.959273\n",
      "Epoch 126, loss: 563.711655\n",
      "Epoch 127, loss: 565.777218\n",
      "Epoch 128, loss: 567.508970\n",
      "Epoch 129, loss: 556.097172\n",
      "Epoch 130, loss: 531.519275\n",
      "Epoch 131, loss: 543.415343\n",
      "Epoch 132, loss: 525.171343\n",
      "Epoch 133, loss: 548.416261\n",
      "Epoch 134, loss: 516.995580\n",
      "Epoch 135, loss: 540.477847\n",
      "Epoch 136, loss: 565.155972\n",
      "Epoch 137, loss: 574.990496\n",
      "Epoch 138, loss: 518.762090\n",
      "Epoch 139, loss: 528.643556\n",
      "Epoch 140, loss: 560.400040\n",
      "Epoch 141, loss: 552.600010\n",
      "Epoch 142, loss: 546.737094\n",
      "Epoch 143, loss: 551.057897\n",
      "Epoch 144, loss: 562.173948\n",
      "Epoch 145, loss: 558.473360\n",
      "Epoch 146, loss: 549.094111\n",
      "Epoch 147, loss: 550.701645\n",
      "Epoch 148, loss: 557.457152\n",
      "Epoch 149, loss: 555.942122\n",
      "Epoch 150, loss: 548.146693\n",
      "Epoch 151, loss: 524.116408\n",
      "Epoch 152, loss: 560.614652\n",
      "Epoch 153, loss: 570.314019\n",
      "Epoch 154, loss: 552.715171\n",
      "Epoch 155, loss: 573.598195\n",
      "Epoch 156, loss: 548.212303\n",
      "Epoch 157, loss: 551.243119\n",
      "Epoch 158, loss: 517.414571\n",
      "Epoch 159, loss: 545.909916\n",
      "Epoch 160, loss: 531.821852\n",
      "Epoch 161, loss: 553.148234\n",
      "Epoch 162, loss: 537.858049\n",
      "Epoch 163, loss: 588.445175\n",
      "Epoch 164, loss: 570.681731\n",
      "Epoch 165, loss: 537.518993\n",
      "Epoch 166, loss: 549.972539\n",
      "Epoch 167, loss: 562.864738\n",
      "Epoch 168, loss: 540.124519\n",
      "Epoch 169, loss: 535.954963\n",
      "Epoch 170, loss: 543.364938\n",
      "Epoch 171, loss: 566.769509\n",
      "Epoch 172, loss: 560.616171\n",
      "Epoch 173, loss: 560.815870\n",
      "Epoch 174, loss: 524.511378\n",
      "Epoch 175, loss: 575.049310\n",
      "Epoch 176, loss: 555.600055\n",
      "Epoch 177, loss: 550.144757\n",
      "Epoch 178, loss: 556.042843\n",
      "Epoch 179, loss: 537.670760\n",
      "Epoch 180, loss: 551.095993\n",
      "Epoch 181, loss: 536.659077\n",
      "Epoch 182, loss: 559.463048\n",
      "Epoch 183, loss: 551.212222\n",
      "Epoch 184, loss: 536.207176\n",
      "Epoch 185, loss: 522.295556\n",
      "Epoch 186, loss: 548.340592\n",
      "Epoch 187, loss: 543.231715\n",
      "Epoch 188, loss: 558.525796\n",
      "Epoch 189, loss: 543.755598\n",
      "Epoch 190, loss: 576.969461\n",
      "Epoch 191, loss: 541.626154\n",
      "Epoch 192, loss: 548.587219\n",
      "Epoch 193, loss: 514.045633\n",
      "Epoch 194, loss: 565.084557\n",
      "Epoch 195, loss: 531.041439\n",
      "Epoch 196, loss: 521.192403\n",
      "Epoch 197, loss: 542.397528\n",
      "Epoch 198, loss: 536.456941\n",
      "Epoch 199, loss: 562.345233\n",
      "Epoch 0, loss: 543.740552\n",
      "Epoch 1, loss: 574.405029\n",
      "Epoch 2, loss: 548.418320\n",
      "Epoch 3, loss: 545.058505\n",
      "Epoch 4, loss: 545.720745\n",
      "Epoch 5, loss: 551.189931\n",
      "Epoch 6, loss: 556.668481\n",
      "Epoch 7, loss: 524.885051\n",
      "Epoch 8, loss: 551.483692\n",
      "Epoch 9, loss: 552.308992\n",
      "Epoch 10, loss: 556.441796\n",
      "Epoch 11, loss: 541.792731\n",
      "Epoch 12, loss: 543.054353\n",
      "Epoch 13, loss: 539.273183\n",
      "Epoch 14, loss: 536.399242\n",
      "Epoch 15, loss: 556.805728\n",
      "Epoch 16, loss: 549.817018\n",
      "Epoch 17, loss: 565.250885\n",
      "Epoch 18, loss: 571.554772\n",
      "Epoch 19, loss: 546.470358\n",
      "Epoch 20, loss: 527.087197\n",
      "Epoch 21, loss: 550.355606\n",
      "Epoch 22, loss: 557.598196\n",
      "Epoch 23, loss: 560.977966\n",
      "Epoch 24, loss: 518.036270\n",
      "Epoch 25, loss: 549.574301\n",
      "Epoch 26, loss: 553.733494\n",
      "Epoch 27, loss: 543.221036\n",
      "Epoch 28, loss: 548.495646\n",
      "Epoch 29, loss: 543.749433\n",
      "Epoch 30, loss: 540.289715\n",
      "Epoch 31, loss: 559.815787\n",
      "Epoch 32, loss: 553.300367\n",
      "Epoch 33, loss: 550.585750\n",
      "Epoch 34, loss: 555.479085\n",
      "Epoch 35, loss: 516.290118\n",
      "Epoch 36, loss: 575.858663\n",
      "Epoch 37, loss: 540.086593\n",
      "Epoch 38, loss: 543.132763\n",
      "Epoch 39, loss: 551.606404\n",
      "Epoch 40, loss: 557.615161\n",
      "Epoch 41, loss: 550.968907\n",
      "Epoch 42, loss: 519.466923\n",
      "Epoch 43, loss: 547.935209\n",
      "Epoch 44, loss: 558.893854\n",
      "Epoch 45, loss: 538.243961\n",
      "Epoch 46, loss: 556.944535\n",
      "Epoch 47, loss: 526.476123\n",
      "Epoch 48, loss: 564.223379\n",
      "Epoch 49, loss: 553.935723\n",
      "Epoch 50, loss: 543.004449\n",
      "Epoch 51, loss: 555.604696\n",
      "Epoch 52, loss: 542.697502\n",
      "Epoch 53, loss: 547.718974\n",
      "Epoch 54, loss: 552.819186\n",
      "Epoch 55, loss: 549.255832\n",
      "Epoch 56, loss: 535.546402\n",
      "Epoch 57, loss: 562.159800\n",
      "Epoch 58, loss: 541.769639\n",
      "Epoch 59, loss: 539.026001\n",
      "Epoch 60, loss: 537.064188\n",
      "Epoch 61, loss: 550.395630\n",
      "Epoch 62, loss: 560.442344\n",
      "Epoch 63, loss: 522.040760\n",
      "Epoch 64, loss: 554.973280\n",
      "Epoch 65, loss: 531.517838\n",
      "Epoch 66, loss: 553.776452\n",
      "Epoch 67, loss: 563.348336\n",
      "Epoch 68, loss: 541.676164\n",
      "Epoch 69, loss: 538.280364\n",
      "Epoch 70, loss: 545.149599\n",
      "Epoch 71, loss: 581.576131\n",
      "Epoch 72, loss: 538.354527\n",
      "Epoch 73, loss: 556.694362\n",
      "Epoch 74, loss: 524.298927\n",
      "Epoch 75, loss: 532.579775\n",
      "Epoch 76, loss: 564.040330\n",
      "Epoch 77, loss: 548.251199\n",
      "Epoch 78, loss: 525.505823\n",
      "Epoch 79, loss: 512.498778\n",
      "Epoch 80, loss: 532.523818\n",
      "Epoch 81, loss: 547.496171\n",
      "Epoch 82, loss: 557.848669\n",
      "Epoch 83, loss: 531.733175\n",
      "Epoch 84, loss: 526.847353\n",
      "Epoch 85, loss: 529.576156\n",
      "Epoch 86, loss: 549.934072\n",
      "Epoch 87, loss: 553.858871\n",
      "Epoch 88, loss: 547.566680\n",
      "Epoch 89, loss: 570.267214\n",
      "Epoch 90, loss: 569.380617\n",
      "Epoch 91, loss: 569.310880\n",
      "Epoch 92, loss: 550.787959\n",
      "Epoch 93, loss: 541.596540\n",
      "Epoch 94, loss: 545.721127\n",
      "Epoch 95, loss: 554.654434\n",
      "Epoch 96, loss: 521.249292\n",
      "Epoch 97, loss: 556.679518\n",
      "Epoch 98, loss: 564.654835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, loss: 553.414861\n",
      "Epoch 100, loss: 569.998874\n",
      "Epoch 101, loss: 541.004987\n",
      "Epoch 102, loss: 546.263808\n",
      "Epoch 103, loss: 539.079723\n",
      "Epoch 104, loss: 565.546030\n",
      "Epoch 105, loss: 537.240468\n",
      "Epoch 106, loss: 544.228558\n",
      "Epoch 107, loss: 543.387303\n",
      "Epoch 108, loss: 579.089639\n",
      "Epoch 109, loss: 558.111157\n",
      "Epoch 110, loss: 571.437966\n",
      "Epoch 111, loss: 546.592445\n",
      "Epoch 112, loss: 570.173901\n",
      "Epoch 113, loss: 557.783735\n",
      "Epoch 114, loss: 558.301909\n",
      "Epoch 115, loss: 554.845351\n",
      "Epoch 116, loss: 551.890566\n",
      "Epoch 117, loss: 546.956948\n",
      "Epoch 118, loss: 546.479448\n",
      "Epoch 119, loss: 542.848612\n",
      "Epoch 120, loss: 548.220091\n",
      "Epoch 121, loss: 555.653771\n",
      "Epoch 122, loss: 582.213917\n",
      "Epoch 123, loss: 558.338652\n",
      "Epoch 124, loss: 571.303392\n",
      "Epoch 125, loss: 552.064536\n",
      "Epoch 126, loss: 550.116210\n",
      "Epoch 127, loss: 534.989752\n",
      "Epoch 128, loss: 511.277919\n",
      "Epoch 129, loss: 547.276059\n",
      "Epoch 130, loss: 550.509711\n",
      "Epoch 131, loss: 531.376613\n",
      "Epoch 132, loss: 582.881342\n",
      "Epoch 133, loss: 560.256984\n",
      "Epoch 134, loss: 564.678696\n",
      "Epoch 135, loss: 544.087227\n",
      "Epoch 136, loss: 540.273538\n",
      "Epoch 137, loss: 550.807076\n",
      "Epoch 138, loss: 558.545398\n",
      "Epoch 139, loss: 553.191135\n",
      "Epoch 140, loss: 557.845798\n",
      "Epoch 141, loss: 564.251808\n",
      "Epoch 142, loss: 538.294340\n",
      "Epoch 143, loss: 556.718951\n",
      "Epoch 144, loss: 558.427221\n",
      "Epoch 145, loss: 529.792214\n",
      "Epoch 146, loss: 547.838704\n",
      "Epoch 147, loss: 550.376212\n",
      "Epoch 148, loss: 581.329984\n",
      "Epoch 149, loss: 547.029587\n",
      "Epoch 150, loss: 560.918427\n",
      "Epoch 151, loss: 530.274712\n",
      "Epoch 152, loss: 561.909161\n",
      "Epoch 153, loss: 565.484690\n",
      "Epoch 154, loss: 552.112213\n",
      "Epoch 155, loss: 558.211759\n",
      "Epoch 156, loss: 543.408882\n",
      "Epoch 157, loss: 558.893577\n",
      "Epoch 158, loss: 532.638421\n",
      "Epoch 159, loss: 567.965039\n",
      "Epoch 160, loss: 554.333081\n",
      "Epoch 161, loss: 549.627561\n",
      "Epoch 162, loss: 543.242840\n",
      "Epoch 163, loss: 557.972357\n",
      "Epoch 164, loss: 564.864546\n",
      "Epoch 165, loss: 565.561675\n",
      "Epoch 166, loss: 573.054620\n",
      "Epoch 167, loss: 543.795718\n",
      "Epoch 168, loss: 533.298018\n",
      "Epoch 169, loss: 545.643724\n",
      "Epoch 170, loss: 549.362685\n",
      "Epoch 171, loss: 544.861183\n",
      "Epoch 172, loss: 525.897113\n",
      "Epoch 173, loss: 530.838198\n",
      "Epoch 174, loss: 542.213109\n",
      "Epoch 175, loss: 530.789035\n",
      "Epoch 176, loss: 551.105669\n",
      "Epoch 177, loss: 587.855497\n",
      "Epoch 178, loss: 550.587479\n",
      "Epoch 179, loss: 576.047544\n",
      "Epoch 180, loss: 552.888807\n",
      "Epoch 181, loss: 562.606434\n",
      "Epoch 182, loss: 558.083100\n",
      "Epoch 183, loss: 553.326179\n",
      "Epoch 184, loss: 566.513002\n",
      "Epoch 185, loss: 541.915010\n",
      "Epoch 186, loss: 546.191246\n",
      "Epoch 187, loss: 554.261496\n",
      "Epoch 188, loss: 546.211055\n",
      "Epoch 189, loss: 532.410574\n",
      "Epoch 190, loss: 547.402643\n",
      "Epoch 191, loss: 592.112939\n",
      "Epoch 192, loss: 561.199782\n",
      "Epoch 193, loss: 532.709158\n",
      "Epoch 194, loss: 564.681887\n",
      "Epoch 195, loss: 529.655168\n",
      "Epoch 196, loss: 580.902585\n",
      "Epoch 197, loss: 550.492487\n",
      "Epoch 198, loss: 557.718962\n",
      "Epoch 199, loss: 537.559715\n",
      "Epoch 0, loss: 565.757830\n",
      "Epoch 1, loss: 553.644448\n",
      "Epoch 2, loss: 575.893073\n",
      "Epoch 3, loss: 564.102361\n",
      "Epoch 4, loss: 541.645492\n",
      "Epoch 5, loss: 553.453931\n",
      "Epoch 6, loss: 534.119342\n",
      "Epoch 7, loss: 539.260292\n",
      "Epoch 8, loss: 531.348936\n",
      "Epoch 9, loss: 559.731121\n",
      "Epoch 10, loss: 554.624019\n",
      "Epoch 11, loss: 533.904663\n",
      "Epoch 12, loss: 555.293009\n",
      "Epoch 13, loss: 532.434467\n",
      "Epoch 14, loss: 569.328672\n",
      "Epoch 15, loss: 532.923703\n",
      "Epoch 16, loss: 576.457591\n",
      "Epoch 17, loss: 564.137483\n",
      "Epoch 18, loss: 574.406873\n",
      "Epoch 19, loss: 528.978037\n",
      "Epoch 20, loss: 544.084164\n",
      "Epoch 21, loss: 566.538528\n",
      "Epoch 22, loss: 526.606178\n",
      "Epoch 23, loss: 546.029834\n",
      "Epoch 24, loss: 559.223695\n",
      "Epoch 25, loss: 562.680521\n",
      "Epoch 26, loss: 529.195257\n",
      "Epoch 27, loss: 541.655084\n",
      "Epoch 28, loss: 584.147111\n",
      "Epoch 29, loss: 533.900645\n",
      "Epoch 30, loss: 561.177330\n",
      "Epoch 31, loss: 538.413718\n",
      "Epoch 32, loss: 569.776408\n",
      "Epoch 33, loss: 514.406216\n",
      "Epoch 34, loss: 551.983897\n",
      "Epoch 35, loss: 558.670382\n",
      "Epoch 36, loss: 530.765860\n",
      "Epoch 37, loss: 539.416759\n",
      "Epoch 38, loss: 537.361107\n",
      "Epoch 39, loss: 564.877517\n",
      "Epoch 40, loss: 532.389256\n",
      "Epoch 41, loss: 550.861232\n",
      "Epoch 42, loss: 551.977504\n",
      "Epoch 43, loss: 559.195895\n",
      "Epoch 44, loss: 544.550771\n",
      "Epoch 45, loss: 519.827639\n",
      "Epoch 46, loss: 557.602280\n",
      "Epoch 47, loss: 561.946519\n",
      "Epoch 48, loss: 574.379105\n",
      "Epoch 49, loss: 530.618218\n",
      "Epoch 50, loss: 549.463790\n",
      "Epoch 51, loss: 551.238040\n",
      "Epoch 52, loss: 567.865636\n",
      "Epoch 53, loss: 521.478954\n",
      "Epoch 54, loss: 571.411398\n",
      "Epoch 55, loss: 547.940693\n",
      "Epoch 56, loss: 558.231571\n",
      "Epoch 57, loss: 531.561862\n",
      "Epoch 58, loss: 541.232359\n",
      "Epoch 59, loss: 555.396614\n",
      "Epoch 60, loss: 538.207329\n",
      "Epoch 61, loss: 556.176739\n",
      "Epoch 62, loss: 554.719933\n",
      "Epoch 63, loss: 563.609020\n",
      "Epoch 64, loss: 574.364438\n",
      "Epoch 65, loss: 542.798648\n",
      "Epoch 66, loss: 558.284939\n",
      "Epoch 67, loss: 543.017658\n",
      "Epoch 68, loss: 567.694404\n",
      "Epoch 69, loss: 573.829675\n",
      "Epoch 70, loss: 551.034468\n",
      "Epoch 71, loss: 541.185281\n",
      "Epoch 72, loss: 524.165587\n",
      "Epoch 73, loss: 543.411025\n",
      "Epoch 74, loss: 538.409544\n",
      "Epoch 75, loss: 539.661840\n",
      "Epoch 76, loss: 533.177217\n",
      "Epoch 77, loss: 564.027056\n",
      "Epoch 78, loss: 551.651969\n",
      "Epoch 79, loss: 554.970570\n",
      "Epoch 80, loss: 550.961908\n",
      "Epoch 81, loss: 544.102487\n",
      "Epoch 82, loss: 533.199564\n",
      "Epoch 83, loss: 582.735507\n",
      "Epoch 84, loss: 553.675593\n",
      "Epoch 85, loss: 576.789698\n",
      "Epoch 86, loss: 557.747769\n",
      "Epoch 87, loss: 537.758778\n",
      "Epoch 88, loss: 529.625803\n",
      "Epoch 89, loss: 555.939017\n",
      "Epoch 90, loss: 536.278919\n",
      "Epoch 91, loss: 521.078981\n",
      "Epoch 92, loss: 521.652672\n",
      "Epoch 93, loss: 565.376165\n",
      "Epoch 94, loss: 579.372597\n",
      "Epoch 95, loss: 518.843324\n",
      "Epoch 96, loss: 555.457441\n",
      "Epoch 97, loss: 565.721207\n",
      "Epoch 98, loss: 570.411971\n",
      "Epoch 99, loss: 562.501553\n",
      "Epoch 100, loss: 531.847921\n",
      "Epoch 101, loss: 555.655858\n",
      "Epoch 102, loss: 551.946582\n",
      "Epoch 103, loss: 559.308659\n",
      "Epoch 104, loss: 547.177415\n",
      "Epoch 105, loss: 553.919212\n",
      "Epoch 106, loss: 531.585818\n",
      "Epoch 107, loss: 565.884602\n",
      "Epoch 108, loss: 525.924221\n",
      "Epoch 109, loss: 547.432604\n",
      "Epoch 110, loss: 533.907093\n",
      "Epoch 111, loss: 518.122130\n",
      "Epoch 112, loss: 564.435486\n",
      "Epoch 113, loss: 542.034737\n",
      "Epoch 114, loss: 539.453113\n",
      "Epoch 115, loss: 552.452555\n",
      "Epoch 116, loss: 556.987131\n",
      "Epoch 117, loss: 539.408110\n",
      "Epoch 118, loss: 558.043371\n",
      "Epoch 119, loss: 547.171992\n",
      "Epoch 120, loss: 526.432014\n",
      "Epoch 121, loss: 569.681988\n",
      "Epoch 122, loss: 535.109898\n",
      "Epoch 123, loss: 566.077255\n",
      "Epoch 124, loss: 576.636430\n",
      "Epoch 125, loss: 551.015291\n",
      "Epoch 126, loss: 546.015095\n",
      "Epoch 127, loss: 549.819929\n",
      "Epoch 128, loss: 548.845798\n",
      "Epoch 129, loss: 529.577141\n",
      "Epoch 130, loss: 563.268649\n",
      "Epoch 131, loss: 546.635290\n",
      "Epoch 132, loss: 517.412459\n",
      "Epoch 133, loss: 556.875704\n",
      "Epoch 134, loss: 531.651826\n",
      "Epoch 135, loss: 562.381779\n",
      "Epoch 136, loss: 573.883339\n",
      "Epoch 137, loss: 567.582572\n",
      "Epoch 138, loss: 539.719921\n",
      "Epoch 139, loss: 563.636612\n",
      "Epoch 140, loss: 560.651046\n",
      "Epoch 141, loss: 559.547794\n",
      "Epoch 142, loss: 520.397309\n",
      "Epoch 143, loss: 517.646150\n",
      "Epoch 144, loss: 528.491954\n",
      "Epoch 145, loss: 548.225749\n",
      "Epoch 146, loss: 550.151740\n",
      "Epoch 147, loss: 550.110348\n",
      "Epoch 148, loss: 540.201916\n",
      "Epoch 149, loss: 548.615950\n",
      "Epoch 150, loss: 535.934096\n",
      "Epoch 151, loss: 540.883059\n",
      "Epoch 152, loss: 552.380934\n",
      "Epoch 153, loss: 541.167870\n",
      "Epoch 154, loss: 533.891574\n",
      "Epoch 155, loss: 565.971405\n",
      "Epoch 156, loss: 529.128424\n",
      "Epoch 157, loss: 530.245831\n",
      "Epoch 158, loss: 538.943234\n",
      "Epoch 159, loss: 545.946196\n",
      "Epoch 160, loss: 563.988589\n",
      "Epoch 161, loss: 527.694164\n",
      "Epoch 162, loss: 519.456880\n",
      "Epoch 163, loss: 530.796143\n",
      "Epoch 164, loss: 530.205319\n",
      "Epoch 165, loss: 570.275422\n",
      "Epoch 166, loss: 532.198591\n",
      "Epoch 167, loss: 565.504838\n",
      "Epoch 168, loss: 550.165716\n",
      "Epoch 169, loss: 549.434157\n",
      "Epoch 170, loss: 566.203893\n",
      "Epoch 171, loss: 551.979810\n",
      "Epoch 172, loss: 571.984974\n",
      "Epoch 173, loss: 562.065409\n",
      "Epoch 174, loss: 560.374754\n",
      "Epoch 175, loss: 528.736898\n",
      "Epoch 176, loss: 548.823646\n",
      "Epoch 177, loss: 523.938620\n",
      "Epoch 178, loss: 533.736930\n",
      "Epoch 179, loss: 553.789219\n",
      "Epoch 180, loss: 555.180669\n",
      "Epoch 181, loss: 557.446672\n",
      "Epoch 182, loss: 545.671137\n",
      "Epoch 183, loss: 542.550470\n",
      "Epoch 184, loss: 555.652587\n",
      "Epoch 185, loss: 569.001406\n",
      "Epoch 186, loss: 545.954431\n",
      "Epoch 187, loss: 559.380593\n",
      "Epoch 188, loss: 543.131538\n",
      "Epoch 189, loss: 536.222211\n",
      "Epoch 190, loss: 546.651756\n",
      "Epoch 191, loss: 563.104400\n",
      "Epoch 192, loss: 552.966583\n",
      "Epoch 193, loss: 566.149797\n",
      "Epoch 194, loss: 545.749518\n",
      "Epoch 195, loss: 545.581063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196, loss: 558.203889\n",
      "Epoch 197, loss: 583.160953\n",
      "Epoch 198, loss: 529.228653\n",
      "Epoch 199, loss: 553.568038\n",
      "Epoch 0, loss: 552.011130\n",
      "Epoch 1, loss: 566.406423\n",
      "Epoch 2, loss: 529.080942\n",
      "Epoch 3, loss: 529.298406\n",
      "Epoch 4, loss: 510.535556\n",
      "Epoch 5, loss: 526.227659\n",
      "Epoch 6, loss: 567.265479\n",
      "Epoch 7, loss: 526.532980\n",
      "Epoch 8, loss: 527.289550\n",
      "Epoch 9, loss: 536.842028\n",
      "Epoch 10, loss: 535.161823\n",
      "Epoch 11, loss: 562.409465\n",
      "Epoch 12, loss: 552.452530\n",
      "Epoch 13, loss: 578.532755\n",
      "Epoch 14, loss: 555.964675\n",
      "Epoch 15, loss: 571.614775\n",
      "Epoch 16, loss: 537.650599\n",
      "Epoch 17, loss: 562.320322\n",
      "Epoch 18, loss: 573.889991\n",
      "Epoch 19, loss: 540.536031\n",
      "Epoch 20, loss: 553.681025\n",
      "Epoch 21, loss: 522.472513\n",
      "Epoch 22, loss: 557.591998\n",
      "Epoch 23, loss: 551.409326\n",
      "Epoch 24, loss: 538.784271\n",
      "Epoch 25, loss: 561.670058\n",
      "Epoch 26, loss: 516.794082\n",
      "Epoch 27, loss: 568.802460\n",
      "Epoch 28, loss: 535.440371\n",
      "Epoch 29, loss: 560.851158\n",
      "Epoch 30, loss: 554.622271\n",
      "Epoch 31, loss: 544.047641\n",
      "Epoch 32, loss: 573.554723\n",
      "Epoch 33, loss: 538.324334\n",
      "Epoch 34, loss: 540.718915\n",
      "Epoch 35, loss: 554.975401\n",
      "Epoch 36, loss: 567.293787\n",
      "Epoch 37, loss: 567.790686\n",
      "Epoch 38, loss: 546.030759\n",
      "Epoch 39, loss: 572.979706\n",
      "Epoch 40, loss: 571.385698\n",
      "Epoch 41, loss: 560.482689\n",
      "Epoch 42, loss: 528.748043\n",
      "Epoch 43, loss: 531.091044\n",
      "Epoch 44, loss: 570.658636\n",
      "Epoch 45, loss: 554.083898\n",
      "Epoch 46, loss: 552.397957\n",
      "Epoch 47, loss: 553.689341\n",
      "Epoch 48, loss: 547.901068\n",
      "Epoch 49, loss: 535.196752\n",
      "Epoch 50, loss: 569.070079\n",
      "Epoch 51, loss: 545.164261\n",
      "Epoch 52, loss: 537.977829\n",
      "Epoch 53, loss: 533.855053\n",
      "Epoch 54, loss: 560.656163\n",
      "Epoch 55, loss: 553.061898\n",
      "Epoch 56, loss: 524.853607\n",
      "Epoch 57, loss: 548.835331\n",
      "Epoch 58, loss: 587.671344\n",
      "Epoch 59, loss: 542.124128\n",
      "Epoch 60, loss: 568.543948\n",
      "Epoch 61, loss: 570.114843\n",
      "Epoch 62, loss: 534.937394\n",
      "Epoch 63, loss: 556.838389\n",
      "Epoch 64, loss: 552.923506\n",
      "Epoch 65, loss: 536.673729\n",
      "Epoch 66, loss: 548.491084\n",
      "Epoch 67, loss: 534.052667\n",
      "Epoch 68, loss: 553.916900\n",
      "Epoch 69, loss: 571.067090\n",
      "Epoch 70, loss: 577.773149\n",
      "Epoch 71, loss: 527.204188\n",
      "Epoch 72, loss: 538.094561\n",
      "Epoch 73, loss: 541.686041\n",
      "Epoch 74, loss: 533.052298\n",
      "Epoch 75, loss: 544.395769\n",
      "Epoch 76, loss: 556.814183\n",
      "Epoch 77, loss: 550.933336\n",
      "Epoch 78, loss: 552.833635\n",
      "Epoch 79, loss: 539.754914\n",
      "Epoch 80, loss: 558.843480\n",
      "Epoch 81, loss: 549.270120\n",
      "Epoch 82, loss: 557.197630\n",
      "Epoch 83, loss: 533.530546\n",
      "Epoch 84, loss: 548.724825\n",
      "Epoch 85, loss: 543.043999\n",
      "Epoch 86, loss: 545.202067\n",
      "Epoch 87, loss: 518.142081\n",
      "Epoch 88, loss: 555.686629\n",
      "Epoch 89, loss: 532.432962\n",
      "Epoch 90, loss: 528.586386\n",
      "Epoch 91, loss: 538.653947\n",
      "Epoch 92, loss: 560.786168\n",
      "Epoch 93, loss: 552.771520\n",
      "Epoch 94, loss: 529.275519\n",
      "Epoch 95, loss: 568.485271\n",
      "Epoch 96, loss: 556.019398\n",
      "Epoch 97, loss: 558.577791\n",
      "Epoch 98, loss: 547.270985\n",
      "Epoch 99, loss: 532.739597\n",
      "Epoch 100, loss: 538.554192\n",
      "Epoch 101, loss: 538.266078\n",
      "Epoch 102, loss: 559.538043\n",
      "Epoch 103, loss: 583.926801\n",
      "Epoch 104, loss: 536.549382\n",
      "Epoch 105, loss: 527.606097\n",
      "Epoch 106, loss: 527.713913\n",
      "Epoch 107, loss: 517.916871\n",
      "Epoch 108, loss: 528.554283\n",
      "Epoch 109, loss: 527.004760\n",
      "Epoch 110, loss: 550.621269\n",
      "Epoch 111, loss: 548.066125\n",
      "Epoch 112, loss: 559.663627\n",
      "Epoch 113, loss: 558.130589\n",
      "Epoch 114, loss: 557.916374\n",
      "Epoch 115, loss: 559.444215\n",
      "Epoch 116, loss: 538.471460\n",
      "Epoch 117, loss: 539.777711\n",
      "Epoch 118, loss: 571.231376\n",
      "Epoch 119, loss: 559.196013\n",
      "Epoch 120, loss: 554.265004\n",
      "Epoch 121, loss: 551.139431\n",
      "Epoch 122, loss: 542.727989\n",
      "Epoch 123, loss: 532.213696\n",
      "Epoch 124, loss: 523.155237\n",
      "Epoch 125, loss: 525.153682\n",
      "Epoch 126, loss: 541.783813\n",
      "Epoch 127, loss: 542.644822\n",
      "Epoch 128, loss: 542.852306\n",
      "Epoch 129, loss: 559.757850\n",
      "Epoch 130, loss: 567.838521\n",
      "Epoch 131, loss: 540.798429\n",
      "Epoch 132, loss: 567.041688\n",
      "Epoch 133, loss: 561.631667\n",
      "Epoch 134, loss: 563.931590\n",
      "Epoch 135, loss: 570.890745\n",
      "Epoch 136, loss: 547.322604\n",
      "Epoch 137, loss: 529.860872\n",
      "Epoch 138, loss: 540.579744\n",
      "Epoch 139, loss: 548.265950\n",
      "Epoch 140, loss: 587.792929\n",
      "Epoch 141, loss: 566.135638\n",
      "Epoch 142, loss: 542.816151\n",
      "Epoch 143, loss: 550.661391\n",
      "Epoch 144, loss: 519.901510\n",
      "Epoch 145, loss: 560.541707\n",
      "Epoch 146, loss: 532.655432\n",
      "Epoch 147, loss: 541.078428\n",
      "Epoch 148, loss: 518.687986\n",
      "Epoch 149, loss: 557.752754\n",
      "Epoch 150, loss: 565.025336\n",
      "Epoch 151, loss: 547.308420\n",
      "Epoch 152, loss: 557.709350\n",
      "Epoch 153, loss: 554.564886\n",
      "Epoch 154, loss: 533.755182\n",
      "Epoch 155, loss: 545.215470\n",
      "Epoch 156, loss: 558.918305\n",
      "Epoch 157, loss: 527.584887\n",
      "Epoch 158, loss: 525.948202\n",
      "Epoch 159, loss: 548.690589\n",
      "Epoch 160, loss: 533.486090\n",
      "Epoch 161, loss: 526.686303\n",
      "Epoch 162, loss: 543.756186\n",
      "Epoch 163, loss: 528.257584\n",
      "Epoch 164, loss: 542.587099\n",
      "Epoch 165, loss: 559.379839\n",
      "Epoch 166, loss: 549.104706\n",
      "Epoch 167, loss: 531.900937\n",
      "Epoch 168, loss: 527.411679\n",
      "Epoch 169, loss: 550.071501\n",
      "Epoch 170, loss: 541.129476\n",
      "Epoch 171, loss: 547.718725\n",
      "Epoch 172, loss: 537.146763\n",
      "Epoch 173, loss: 566.970047\n",
      "Epoch 174, loss: 537.633376\n",
      "Epoch 175, loss: 542.398722\n",
      "Epoch 176, loss: 553.706187\n",
      "Epoch 177, loss: 576.444100\n",
      "Epoch 178, loss: 547.211782\n",
      "Epoch 179, loss: 560.828079\n",
      "Epoch 180, loss: 535.066815\n",
      "Epoch 181, loss: 526.537590\n",
      "Epoch 182, loss: 509.938366\n",
      "Epoch 183, loss: 581.767467\n",
      "Epoch 184, loss: 555.302885\n",
      "Epoch 185, loss: 519.995101\n",
      "Epoch 186, loss: 555.552597\n",
      "Epoch 187, loss: 565.277359\n",
      "Epoch 188, loss: 523.278515\n",
      "Epoch 189, loss: 518.496636\n",
      "Epoch 190, loss: 546.667580\n",
      "Epoch 191, loss: 548.020782\n",
      "Epoch 192, loss: 543.957669\n",
      "Epoch 193, loss: 559.226989\n",
      "Epoch 194, loss: 544.935648\n",
      "Epoch 195, loss: 539.096858\n",
      "Epoch 196, loss: 554.892885\n",
      "Epoch 197, loss: 548.377052\n",
      "Epoch 198, loss: 520.210293\n",
      "Epoch 199, loss: 555.197706\n",
      "Epoch 0, loss: 553.497285\n",
      "Epoch 1, loss: 573.808909\n",
      "Epoch 2, loss: 586.408757\n",
      "Epoch 3, loss: 536.550397\n",
      "Epoch 4, loss: 564.922018\n",
      "Epoch 5, loss: 524.492961\n",
      "Epoch 6, loss: 540.859216\n",
      "Epoch 7, loss: 566.055941\n",
      "Epoch 8, loss: 573.849632\n",
      "Epoch 9, loss: 534.900562\n",
      "Epoch 10, loss: 546.501386\n",
      "Epoch 11, loss: 548.944590\n",
      "Epoch 12, loss: 557.090507\n",
      "Epoch 13, loss: 540.476490\n",
      "Epoch 14, loss: 528.000810\n",
      "Epoch 15, loss: 550.224609\n",
      "Epoch 16, loss: 550.008061\n",
      "Epoch 17, loss: 539.814892\n",
      "Epoch 18, loss: 545.522686\n",
      "Epoch 19, loss: 531.995553\n",
      "Epoch 20, loss: 544.933148\n",
      "Epoch 21, loss: 550.730493\n",
      "Epoch 22, loss: 547.911911\n",
      "Epoch 23, loss: 539.648539\n",
      "Epoch 24, loss: 557.935057\n",
      "Epoch 25, loss: 565.568451\n",
      "Epoch 26, loss: 540.880717\n",
      "Epoch 27, loss: 551.704521\n",
      "Epoch 28, loss: 550.242980\n",
      "Epoch 29, loss: 546.512079\n",
      "Epoch 30, loss: 531.944341\n",
      "Epoch 31, loss: 542.923797\n",
      "Epoch 32, loss: 560.638543\n",
      "Epoch 33, loss: 554.384924\n",
      "Epoch 34, loss: 548.140794\n",
      "Epoch 35, loss: 557.891379\n",
      "Epoch 36, loss: 558.398627\n",
      "Epoch 37, loss: 560.457911\n",
      "Epoch 38, loss: 543.589924\n",
      "Epoch 39, loss: 527.107176\n",
      "Epoch 40, loss: 548.553772\n",
      "Epoch 41, loss: 564.345022\n",
      "Epoch 42, loss: 551.494788\n",
      "Epoch 43, loss: 518.040351\n",
      "Epoch 44, loss: 547.942383\n",
      "Epoch 45, loss: 563.328780\n",
      "Epoch 46, loss: 538.976228\n",
      "Epoch 47, loss: 566.164136\n",
      "Epoch 48, loss: 549.047631\n",
      "Epoch 49, loss: 558.147146\n",
      "Epoch 50, loss: 547.848421\n",
      "Epoch 51, loss: 564.371613\n",
      "Epoch 52, loss: 540.335265\n",
      "Epoch 53, loss: 547.239171\n",
      "Epoch 54, loss: 562.112656\n",
      "Epoch 55, loss: 551.975625\n",
      "Epoch 56, loss: 541.011716\n",
      "Epoch 57, loss: 529.716420\n",
      "Epoch 58, loss: 573.929483\n",
      "Epoch 59, loss: 557.094857\n",
      "Epoch 60, loss: 530.218479\n",
      "Epoch 61, loss: 539.314416\n",
      "Epoch 62, loss: 501.108409\n",
      "Epoch 63, loss: 548.393211\n",
      "Epoch 64, loss: 551.523642\n",
      "Epoch 65, loss: 565.131208\n",
      "Epoch 66, loss: 552.264561\n",
      "Epoch 67, loss: 547.099924\n",
      "Epoch 68, loss: 533.177881\n",
      "Epoch 69, loss: 548.029114\n",
      "Epoch 70, loss: 542.671178\n",
      "Epoch 71, loss: 572.716039\n",
      "Epoch 72, loss: 538.657326\n",
      "Epoch 73, loss: 553.825357\n",
      "Epoch 74, loss: 544.865328\n",
      "Epoch 75, loss: 532.969746\n",
      "Epoch 76, loss: 554.205117\n",
      "Epoch 77, loss: 539.617430\n",
      "Epoch 78, loss: 527.693786\n",
      "Epoch 79, loss: 564.754768\n",
      "Epoch 80, loss: 551.001431\n",
      "Epoch 81, loss: 556.125079\n",
      "Epoch 82, loss: 516.154976\n",
      "Epoch 83, loss: 537.557777\n",
      "Epoch 84, loss: 535.796285\n",
      "Epoch 85, loss: 540.738447\n",
      "Epoch 86, loss: 545.578172\n",
      "Epoch 87, loss: 541.865487\n",
      "Epoch 88, loss: 550.045179\n",
      "Epoch 89, loss: 555.426849\n",
      "Epoch 90, loss: 553.059688\n",
      "Epoch 91, loss: 533.201071\n",
      "Epoch 92, loss: 529.673808\n",
      "Epoch 93, loss: 563.846019\n",
      "Epoch 94, loss: 551.802531\n",
      "Epoch 95, loss: 544.427472\n",
      "Epoch 96, loss: 573.940663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, loss: 545.617651\n",
      "Epoch 98, loss: 546.388095\n",
      "Epoch 99, loss: 533.221822\n",
      "Epoch 100, loss: 571.539857\n",
      "Epoch 101, loss: 530.900877\n",
      "Epoch 102, loss: 539.728310\n",
      "Epoch 103, loss: 549.178998\n",
      "Epoch 104, loss: 538.781193\n",
      "Epoch 105, loss: 537.175567\n",
      "Epoch 106, loss: 554.821880\n",
      "Epoch 107, loss: 557.321858\n",
      "Epoch 108, loss: 544.173063\n",
      "Epoch 109, loss: 536.339113\n",
      "Epoch 110, loss: 537.128480\n",
      "Epoch 111, loss: 554.908538\n",
      "Epoch 112, loss: 573.315745\n",
      "Epoch 113, loss: 563.130606\n",
      "Epoch 114, loss: 547.211213\n",
      "Epoch 115, loss: 564.818466\n",
      "Epoch 116, loss: 535.392723\n",
      "Epoch 117, loss: 555.409221\n",
      "Epoch 118, loss: 552.520228\n",
      "Epoch 119, loss: 567.342611\n",
      "Epoch 120, loss: 572.158829\n",
      "Epoch 121, loss: 533.328016\n",
      "Epoch 122, loss: 551.107518\n",
      "Epoch 123, loss: 562.427801\n",
      "Epoch 124, loss: 540.857729\n",
      "Epoch 125, loss: 534.392210\n",
      "Epoch 126, loss: 523.022996\n",
      "Epoch 127, loss: 537.063304\n",
      "Epoch 128, loss: 515.285625\n",
      "Epoch 129, loss: 562.882297\n",
      "Epoch 130, loss: 544.387887\n",
      "Epoch 131, loss: 542.105374\n",
      "Epoch 132, loss: 551.892014\n",
      "Epoch 133, loss: 552.341369\n",
      "Epoch 134, loss: 559.534875\n",
      "Epoch 135, loss: 534.030636\n",
      "Epoch 136, loss: 552.045481\n",
      "Epoch 137, loss: 557.508620\n",
      "Epoch 138, loss: 559.414312\n",
      "Epoch 139, loss: 537.520425\n",
      "Epoch 140, loss: 580.669152\n",
      "Epoch 141, loss: 563.976186\n",
      "Epoch 142, loss: 544.787153\n",
      "Epoch 143, loss: 547.282874\n",
      "Epoch 144, loss: 546.429179\n",
      "Epoch 145, loss: 555.503541\n",
      "Epoch 146, loss: 570.531973\n",
      "Epoch 147, loss: 547.561439\n",
      "Epoch 148, loss: 532.489653\n",
      "Epoch 149, loss: 562.627340\n",
      "Epoch 150, loss: 547.537832\n",
      "Epoch 151, loss: 505.624422\n",
      "Epoch 152, loss: 548.202177\n",
      "Epoch 153, loss: 523.783322\n",
      "Epoch 154, loss: 530.545107\n",
      "Epoch 155, loss: 551.332185\n",
      "Epoch 156, loss: 553.937148\n",
      "Epoch 157, loss: 558.495937\n",
      "Epoch 158, loss: 535.499413\n",
      "Epoch 159, loss: 544.965838\n",
      "Epoch 160, loss: 550.496092\n",
      "Epoch 161, loss: 535.735896\n",
      "Epoch 162, loss: 547.560772\n",
      "Epoch 163, loss: 563.440506\n",
      "Epoch 164, loss: 543.169326\n",
      "Epoch 165, loss: 549.851571\n",
      "Epoch 166, loss: 552.782990\n",
      "Epoch 167, loss: 544.181551\n",
      "Epoch 168, loss: 540.770166\n",
      "Epoch 169, loss: 569.424225\n",
      "Epoch 170, loss: 534.072301\n",
      "Epoch 171, loss: 511.952993\n",
      "Epoch 172, loss: 536.924900\n",
      "Epoch 173, loss: 532.761793\n",
      "Epoch 174, loss: 530.914819\n",
      "Epoch 175, loss: 534.455667\n",
      "Epoch 176, loss: 528.116708\n",
      "Epoch 177, loss: 544.912507\n",
      "Epoch 178, loss: 533.591378\n",
      "Epoch 179, loss: 542.314000\n",
      "Epoch 180, loss: 551.003205\n",
      "Epoch 181, loss: 546.898471\n",
      "Epoch 182, loss: 535.906901\n",
      "Epoch 183, loss: 559.989025\n",
      "Epoch 184, loss: 549.175212\n",
      "Epoch 185, loss: 575.516458\n",
      "Epoch 186, loss: 561.288239\n",
      "Epoch 187, loss: 527.401182\n",
      "Epoch 188, loss: 537.679889\n",
      "Epoch 189, loss: 543.875162\n",
      "Epoch 190, loss: 562.437670\n",
      "Epoch 191, loss: 551.510375\n",
      "Epoch 192, loss: 528.522947\n",
      "Epoch 193, loss: 559.087074\n",
      "Epoch 194, loss: 537.551395\n",
      "Epoch 195, loss: 522.158243\n",
      "Epoch 196, loss: 532.762287\n",
      "Epoch 197, loss: 549.451720\n",
      "Epoch 198, loss: 534.319553\n",
      "Epoch 199, loss: 559.104805\n",
      "Epoch 0, loss: 535.593331\n",
      "Epoch 1, loss: 572.340933\n",
      "Epoch 2, loss: 521.504287\n",
      "Epoch 3, loss: 528.382228\n",
      "Epoch 4, loss: 545.229765\n",
      "Epoch 5, loss: 550.294117\n",
      "Epoch 6, loss: 549.399791\n",
      "Epoch 7, loss: 558.245408\n",
      "Epoch 8, loss: 530.967601\n",
      "Epoch 9, loss: 541.003121\n",
      "Epoch 10, loss: 543.498162\n",
      "Epoch 11, loss: 538.901194\n",
      "Epoch 12, loss: 541.045963\n",
      "Epoch 13, loss: 554.801875\n",
      "Epoch 14, loss: 555.935884\n",
      "Epoch 15, loss: 562.302665\n",
      "Epoch 16, loss: 574.016059\n",
      "Epoch 17, loss: 579.768847\n",
      "Epoch 18, loss: 536.874262\n",
      "Epoch 19, loss: 525.762401\n",
      "Epoch 20, loss: 560.977643\n",
      "Epoch 21, loss: 552.850105\n",
      "Epoch 22, loss: 563.616584\n",
      "Epoch 23, loss: 545.505468\n",
      "Epoch 24, loss: 542.870879\n",
      "Epoch 25, loss: 523.066626\n",
      "Epoch 26, loss: 537.470666\n",
      "Epoch 27, loss: 567.254048\n",
      "Epoch 28, loss: 541.672717\n",
      "Epoch 29, loss: 555.888159\n",
      "Epoch 30, loss: 535.605483\n",
      "Epoch 31, loss: 538.317112\n",
      "Epoch 32, loss: 553.692614\n",
      "Epoch 33, loss: 529.037722\n",
      "Epoch 34, loss: 538.809123\n",
      "Epoch 35, loss: 554.233304\n",
      "Epoch 36, loss: 534.607073\n",
      "Epoch 37, loss: 553.991396\n",
      "Epoch 38, loss: 530.464278\n",
      "Epoch 39, loss: 530.709074\n",
      "Epoch 40, loss: 543.732917\n",
      "Epoch 41, loss: 561.243209\n",
      "Epoch 42, loss: 527.527380\n",
      "Epoch 43, loss: 527.846468\n",
      "Epoch 44, loss: 539.450695\n",
      "Epoch 45, loss: 553.859868\n",
      "Epoch 46, loss: 526.227090\n",
      "Epoch 47, loss: 554.137229\n",
      "Epoch 48, loss: 561.058575\n",
      "Epoch 49, loss: 530.504203\n",
      "Epoch 50, loss: 544.197827\n",
      "Epoch 51, loss: 543.044914\n",
      "Epoch 52, loss: 554.017450\n",
      "Epoch 53, loss: 528.396442\n",
      "Epoch 54, loss: 518.602134\n",
      "Epoch 55, loss: 541.043363\n",
      "Epoch 56, loss: 537.126493\n",
      "Epoch 57, loss: 564.546948\n",
      "Epoch 58, loss: 536.771878\n",
      "Epoch 59, loss: 533.295149\n",
      "Epoch 60, loss: 515.791563\n",
      "Epoch 61, loss: 550.973653\n",
      "Epoch 62, loss: 553.882651\n",
      "Epoch 63, loss: 558.678394\n",
      "Epoch 64, loss: 549.930423\n",
      "Epoch 65, loss: 546.681727\n",
      "Epoch 66, loss: 555.237399\n",
      "Epoch 67, loss: 530.875098\n",
      "Epoch 68, loss: 519.306879\n",
      "Epoch 69, loss: 521.021558\n",
      "Epoch 70, loss: 550.337477\n",
      "Epoch 71, loss: 562.544295\n",
      "Epoch 72, loss: 567.433150\n",
      "Epoch 73, loss: 540.719596\n",
      "Epoch 74, loss: 556.464047\n",
      "Epoch 75, loss: 543.786601\n",
      "Epoch 76, loss: 545.874257\n",
      "Epoch 77, loss: 545.661128\n",
      "Epoch 78, loss: 547.116091\n",
      "Epoch 79, loss: 541.262695\n",
      "Epoch 80, loss: 520.618443\n",
      "Epoch 81, loss: 540.376502\n",
      "Epoch 82, loss: 535.690921\n",
      "Epoch 83, loss: 535.191969\n",
      "Epoch 84, loss: 550.379926\n",
      "Epoch 85, loss: 563.702626\n",
      "Epoch 86, loss: 551.886992\n",
      "Epoch 87, loss: 540.019341\n",
      "Epoch 88, loss: 531.822919\n",
      "Epoch 89, loss: 553.305286\n",
      "Epoch 90, loss: 539.876621\n",
      "Epoch 91, loss: 537.821197\n",
      "Epoch 92, loss: 562.473942\n",
      "Epoch 93, loss: 551.424539\n",
      "Epoch 94, loss: 539.640160\n",
      "Epoch 95, loss: 548.768483\n",
      "Epoch 96, loss: 551.204538\n",
      "Epoch 97, loss: 546.260084\n",
      "Epoch 98, loss: 566.750061\n",
      "Epoch 99, loss: 549.295471\n",
      "Epoch 100, loss: 551.387511\n",
      "Epoch 101, loss: 541.826493\n",
      "Epoch 102, loss: 541.881330\n",
      "Epoch 103, loss: 531.605943\n",
      "Epoch 104, loss: 528.129391\n",
      "Epoch 105, loss: 554.955926\n",
      "Epoch 106, loss: 562.338008\n",
      "Epoch 107, loss: 554.663809\n",
      "Epoch 108, loss: 529.141493\n",
      "Epoch 109, loss: 520.406940\n",
      "Epoch 110, loss: 522.874423\n",
      "Epoch 111, loss: 550.553348\n",
      "Epoch 112, loss: 541.680205\n",
      "Epoch 113, loss: 557.464326\n",
      "Epoch 114, loss: 547.357396\n",
      "Epoch 115, loss: 534.118488\n",
      "Epoch 116, loss: 523.517073\n",
      "Epoch 117, loss: 540.628742\n",
      "Epoch 118, loss: 554.863176\n",
      "Epoch 119, loss: 535.496527\n",
      "Epoch 120, loss: 535.469254\n",
      "Epoch 121, loss: 578.019066\n",
      "Epoch 122, loss: 530.704451\n",
      "Epoch 123, loss: 558.103984\n",
      "Epoch 124, loss: 551.613072\n",
      "Epoch 125, loss: 525.051092\n",
      "Epoch 126, loss: 550.063684\n",
      "Epoch 127, loss: 540.276202\n",
      "Epoch 128, loss: 541.300844\n",
      "Epoch 129, loss: 554.552129\n",
      "Epoch 130, loss: 527.266226\n",
      "Epoch 131, loss: 549.531305\n",
      "Epoch 132, loss: 536.601011\n",
      "Epoch 133, loss: 541.220377\n",
      "Epoch 134, loss: 542.551257\n",
      "Epoch 135, loss: 565.857549\n",
      "Epoch 136, loss: 559.421918\n",
      "Epoch 137, loss: 528.645179\n",
      "Epoch 138, loss: 550.183127\n",
      "Epoch 139, loss: 560.809136\n",
      "Epoch 140, loss: 529.468614\n",
      "Epoch 141, loss: 535.792223\n",
      "Epoch 142, loss: 547.691776\n",
      "Epoch 143, loss: 545.549050\n",
      "Epoch 144, loss: 536.145174\n",
      "Epoch 145, loss: 539.033678\n",
      "Epoch 146, loss: 518.243895\n",
      "Epoch 147, loss: 531.076039\n",
      "Epoch 148, loss: 560.313568\n",
      "Epoch 149, loss: 534.661824\n",
      "Epoch 150, loss: 555.792089\n",
      "Epoch 151, loss: 553.010526\n",
      "Epoch 152, loss: 549.549839\n",
      "Epoch 153, loss: 575.322864\n",
      "Epoch 154, loss: 530.664622\n",
      "Epoch 155, loss: 551.571187\n",
      "Epoch 156, loss: 537.500895\n",
      "Epoch 157, loss: 544.106196\n",
      "Epoch 158, loss: 516.206217\n",
      "Epoch 159, loss: 516.874692\n",
      "Epoch 160, loss: 532.020774\n",
      "Epoch 161, loss: 525.650852\n",
      "Epoch 162, loss: 532.811348\n",
      "Epoch 163, loss: 552.545290\n",
      "Epoch 164, loss: 553.221977\n",
      "Epoch 165, loss: 557.078603\n",
      "Epoch 166, loss: 557.970862\n",
      "Epoch 167, loss: 556.369217\n",
      "Epoch 168, loss: 530.189615\n",
      "Epoch 169, loss: 531.725601\n",
      "Epoch 170, loss: 533.824695\n",
      "Epoch 171, loss: 542.627461\n",
      "Epoch 172, loss: 549.310167\n",
      "Epoch 173, loss: 534.327629\n",
      "Epoch 174, loss: 549.717866\n",
      "Epoch 175, loss: 543.446975\n",
      "Epoch 176, loss: 554.007490\n",
      "Epoch 177, loss: 541.481477\n",
      "Epoch 178, loss: 545.755721\n",
      "Epoch 179, loss: 553.477475\n",
      "Epoch 180, loss: 537.032488\n",
      "Epoch 181, loss: 536.973107\n",
      "Epoch 182, loss: 574.165334\n",
      "Epoch 183, loss: 561.461848\n",
      "Epoch 184, loss: 553.608113\n",
      "Epoch 185, loss: 542.213937\n",
      "Epoch 186, loss: 547.446602\n",
      "Epoch 187, loss: 557.720931\n",
      "Epoch 188, loss: 515.110306\n",
      "Epoch 189, loss: 539.127077\n",
      "Epoch 190, loss: 549.342223\n",
      "Epoch 191, loss: 562.270596\n",
      "Epoch 192, loss: 546.232916\n",
      "Epoch 193, loss: 556.359075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, loss: 546.564346\n",
      "Epoch 195, loss: 546.365476\n",
      "Epoch 196, loss: 539.518660\n",
      "Epoch 197, loss: 561.790081\n",
      "Epoch 198, loss: 546.510246\n",
      "Epoch 199, loss: 564.933015\n",
      "best validation accuracy achieved: 0.250000\n",
      "Wall time: 8min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from itertools import product\n",
    "\n",
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for learning_rate, reg_strength in product(learning_rates, reg_strengths):\n",
    "    classifier.fit(train_X, train_y, \n",
    "                   epochs=num_epochs, \n",
    "                   learning_rate=learning_rate, \n",
    "                   batch_size=batch_size, \n",
    "                   reg=reg_strength)\n",
    "    \n",
    "    pred = classifier.predict(val_X)\n",
    "    accuracy = multiclass_accuracy(pred, val_y)\n",
    "    \n",
    "    if accuracy > best_val_accuracy:\n",
    "        best_classifier = classifier\n",
    "        best_val_accuracy = accuracy\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.189000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
